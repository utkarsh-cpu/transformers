{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Includes all libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from utilities import * \n",
    "import matplotlib.pyplot as plt\n",
    "import torch as th\n",
    "import Layer as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Linear Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RElu:\n",
    "    def __init__(self,inplace:bool = False):\n",
    "        self.inplace=inplace\n",
    "    \n",
    "    def __call__(self, input: th.Tensor):\n",
    "        if self.inplace:\n",
    "            input=input*(input>0)\n",
    "            return input\n",
    "        else:\n",
    "            tmp=input*(input>0)\n",
    "            return tmp\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self,inplace:bool = False):\n",
    "        self.inplace=inplace\n",
    "    \n",
    "    def __call__(self, input: th.Tensor):\n",
    "        if self.inplace:\n",
    "            input=1 / (1 + th.exp(-input))\n",
    "            return input\n",
    "        else:\n",
    "            tmp=1 / (1 + th.exp(-input))\n",
    "            return tmp\n",
    "\n",
    "class LRElu:\n",
    "    def __init__(self,inplace:bool = False):\n",
    "        self.inplace=inplace\n",
    "    \n",
    "    def __call__(self, input: th.Tensor,alpha:float):\n",
    "        if self.inplace:\n",
    "            input=th.maximum(input,input*alpha)\n",
    "        else:\n",
    "            tmp=th.maximum(input,input*alpha)\n",
    "            return tmp\n",
    "\n",
    "class Elu:\n",
    "    def __init__(self,inplace:bool = False):\n",
    "        self.inplace=inplace\n",
    "    \n",
    "    def __call__(self, input: th.Tensor,alpha:float):\n",
    "        if self.inplace:\n",
    "            input=th.maximum(input,(th.exp(input)-1)*alpha)\n",
    "        else:\n",
    "            tmp=th.maximum(input,(th.exp(input)-1)*alpha)\n",
    "            return tmp\n",
    "\n",
    "class HardSigmoid:\n",
    "    def __init__(self,inplace:bool = False):\n",
    "        self.inplace=inplace\n",
    "    \n",
    "    def __call__(self, input: th.Tensor):\n",
    "        if self.inplace:\n",
    "            input=((input/6)+0.5)*(th.logical_and(input<=3,input>=-3))+input*(th.logical_and(input>=3,input<=-3))\n",
    "        else:\n",
    "            tmp=((input/6)+0.5)*(th.logical_and(input<=3,input>=-3))+input*(th.logical_and(input>=3,input<=-3))\n",
    "            return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various Types of Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention:\n",
    "    def __init__(self,drop):\n",
    "        self.dropout=nn.Dropout(drop)\n",
    "\n",
    "    def __call__(self,q:th.Tensor,k:th.Tensor,v:th.Tensor,valid_lens=None):\n",
    "        d = q.shape[-1]\n",
    "        scores=th.bmm(q,k.transpose(1,2))/th.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return th.bmm(self.dropout(self.attention_weights),v)\n",
    "\n",
    "class AdditiveAttention:\n",
    "    def __init__(self,num_hiddens,drop):\n",
    "        self.linearq=nn.LazyLinear(num_hiddens,bias=False)\n",
    "        self.lineark=nn.LazyLinear(num_hiddens,bias=False)\n",
    "        self.linearv=nn.LazyLinear(1,bias=False)\n",
    "        self.dropout=nn.Dropout(drop)\n",
    "\n",
    "    def __call__(self,q:th.Tensor,k:th.Tensor,v:th.Tensor,valid_lens:th.Tensor=None):\n",
    "        queries,keys=self.linear1(q),self.linear2(k)\n",
    "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
    "        features = th.tanh(features)\n",
    "        scores = self.linear3(features).squeeze(-1)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return th.bmm(self.dropout(self.attention_weights),v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muti Headed Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self,num_heads,num_hiddens,drop,bias=False,**kwargs):\n",
    "        self.num_heads=num_heads\n",
    "        self.attention=ScaledDotProductAttention(drop)\n",
    "        self.linearq=nn.LazyLinear(num_hiddens,bias)\n",
    "        self.lineark=nn.LazyLinear(num_hiddens,bias)\n",
    "        self.linearv=nn.LazyLinear(num_hiddens,bias)\n",
    "        self.linearo=nn.LazyLinear(num_hiddens,bias)\n",
    "    \n",
    "    def transpose_qkv(self,X):\n",
    "        X = X.reshape(X.shape[0],X.shape[1],self.num_heads,-1)\n",
    "        X = X.permute(0,2,1,3)\n",
    "        return X.reshape(-1,X.shape[2],X.shape[1])\n",
    "    \n",
    "    def transpose_output(self,X):\n",
    "        X = X.reshape(X.shape[0],X.shape[1],self.num_heads,-1)\n",
    "        X = X.permute(0,2,1,3)\n",
    "        return X.reshape(-1,X.shape[2],X.shape[3])\n",
    "    \n",
    "    def transpose_output(self,X):\n",
    "        X = X.reshape(-1,self.num_heads,X.shape[1],X.shape[2])\n",
    "        X = X.permute(0,2,1,3)\n",
    "        return X.reshape(X.shape[0],X.shape[1],-1)\n",
    "\n",
    "    def __call__(self,q:th.Tensor,k:th.Tensor,v:th.Tensor,valid_lens):\n",
    "\n",
    "        queries=self.transpose_qkv(self.linearq(q))\n",
    "        key=self.transpose_qkv(self.lineark(k))\n",
    "        values=self.transpose_qkv(self.linearv(v))\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            valid_lens = th.repeat_interleave(\n",
    "                valid_lens, repeats=self.num_heads, dim=0)\n",
    "        self.attentionweights=self.attention(queries,key,values,valid_lens)\n",
    "        output_concat=self.transpose_output(self.attentionweights)\n",
    "        return self.linearo(output_concat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__(self,normalized_shape, eps=1e-05, elementwise_affine: bool=True, bias: bool=True):\n",
    "        self.shape=tuple(normalized_shape)\n",
    "        self.eps=eps\n",
    "        self.elementwise_affine=elementwise_affine\n",
    "        self.gamma=th.ones(normalized_shape)\n",
    "        self.beta=th.zeros(normalized_shape)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
  },
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
